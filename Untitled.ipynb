{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ['a', 'about', 'above', 'across', 'after', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'among', 'an', 'and', 'another', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'are', 'area', 'areas', 'around', 'as', 'ask', 'asked', 'asking', 'asks', 'at', 'away', 'b', 'back', 'backed', 'backing', 'backs', 'be', 'became', 'because', 'become', 'becomes', 'been', 'before', 'began', 'behind', 'being', 'beings', 'best', 'better', 'between', 'big', 'both', 'but', 'by', 'c', 'came', 'can', 'cannot', 'case', 'cases', 'certain', 'certainly', 'clear', 'clearly', 'come', 'could', 'd', 'did', 'differ', 'different', 'differently', 'do', 'does', 'done', 'down', 'down', 'downed', 'downing', 'downs', 'during', 'e', 'each', 'early', 'either', 'end', 'ended', 'ending', 'ends', 'enough', 'even', 'evenly', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'f', 'face', 'faces', 'fact', 'facts', 'far', 'felt', 'few', 'find', 'finds', 'first', 'for', 'four', 'from', 'full', 'fully', 'further', 'furthered', 'furthering', 'furthers', 'g', 'gave', 'general', 'generally', 'get', 'gets', 'give', 'given', 'gives', 'go', 'going', 'good', 'goods', 'got', 'great', 'greater', 'greatest', 'group', 'grouped', 'grouping', 'groups', 'h', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'herself', 'high', 'high', 'high', 'higher', 'highest', 'him', 'himself', 'his', 'how', 'however', 'i', 'if', 'important', 'in', 'interest', 'interested', 'interesting', 'interests', 'into', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kind', 'knew', 'know', 'known', 'knows', 'l', 'large', 'largely', 'last', 'later', 'latest', 'least', 'less', 'let', 'lets', 'like', 'likely', 'long', 'longer', 'longest', 'm', 'made', 'make', 'making', 'man', 'many', 'may', 'me', 'member', 'members', 'men', 'might', 'more', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', 'n', 'necessary', 'need', 'needed', 'needing', 'needs', 'never', 'new', 'new', 'newer', 'newest', 'next', 'no', 'nobody', 'non', 'noone', 'not', 'nothing', 'now', 'nowhere', 'number', 'numbers', 'o', 'of', 'off', 'often', 'old', 'older', 'oldest', 'on', 'once', 'one', 'only', 'open', 'opened', 'opening', 'opens', 'or', 'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'our', 'out', 'over', 'p', 'part', 'parted', 'parting', 'parts', 'per', 'perhaps', 'place', 'places', 'point', 'pointed', 'pointing', 'points', 'possible', 'present', 'presented', 'presenting', 'presents', 'problem', 'problems', 'put', 'puts', 'q', 'quite', 'r', 'rather', 'really', 'right', 'right', 'room', 'rooms', 's', 'said', 'same', 'saw', 'say', 'says', 'second', 'seconds', 'see', 'seem', 'seemed', 'seeming', 'seems', 'sees', 'several', 'shall', 'she', 'should', 'show', 'showed', 'showing', 'shows', 'side', 'sides', 'since', 'small', 'smaller', 'smallest', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', 'state', 'states', 'still', 'still', 'such', 'sure', 't', 'take', 'taken', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'these', 'they', 'thing', 'things', 'think', 'thinks', 'this', 'those', 'though', 'thought', 'thoughts', 'three', 'through', 'thus', 'to', 'today', 'together', 'too', 'took', 'toward', 'turn', 'turned', 'turning', 'turns', 'two', 'u', 'under', 'until', 'up', 'upon', 'us', 'use', 'used', 'uses', 'v', 'very', 'w', 'want', 'wanted', 'wanting', 'wants', 'was', 'way', 'ways', 'we', 'well', 'wells', 'went', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with', 'within', 'without', 'work', 'worked', 'working', 'works', 'would', 'x', 'y', 'year', 'years', 'yet', 'you', 'young', 'younger', 'youngest', 'your', 'yours', 'z']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import urllib2\n",
    "import string\n",
    "import os\n",
    "\n",
    "stop_word_list = []\n",
    "f = open('stop_word_file.txt', 'r')\n",
    "for line in f.readlines():\n",
    "    line = line.replace(\"\\n\",\"\")\n",
    "    stop_word_list.append(line)\n",
    "f.close()\n",
    "\n",
    "print stop_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site Name : http://edition.cnn.com\n",
      "\n",
      "단어 종류 : 1480\n",
      "\n",
      "Site Name : http://www.nike.com\n",
      "\n",
      "단어 종류 : 93\n",
      "\n",
      "Site Name : https://www.oracle.com\n",
      "\n",
      "단어 종류 : 423\n",
      "\n",
      "Site Name : http://mlb.mlb.com\n",
      "\n",
      "단어 종류 : 1520\n",
      "\n",
      "Site Name : https://www.whitehouse.gov\n",
      "\n",
      "단어 종류 : 267\n",
      "\n",
      "http://edition.cnn.com\n",
      "http://www.nike.com\n",
      "https://www.oracle.com\n",
      "http://mlb.mlb.com\n",
      "https://www.whitehouse.gov\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import urllib2\n",
    "import string\n",
    "\n",
    "def word_list_dic(site):\n",
    "    source = urllib2.urlopen(site).read()\n",
    "\n",
    "    #print source \n",
    "    source_list = source.split(\"\\n\")\n",
    "    sentence_list = []\n",
    "    s_l = []\n",
    "    #n = 0\n",
    "\n",
    "    for line in source_list:\n",
    "        line = line.strip()\n",
    "        if line == \" \" or line == '':\n",
    "            continue\n",
    "        s_l.append(line)\n",
    "\n",
    "    for line in s_l: \n",
    "        #print \"%d : %s \"% (n, line)\n",
    "        #n += 1\n",
    "        check = False\n",
    "        sentence = \"\"\n",
    "        for i in line:\n",
    "            #print i\n",
    "            if '<' == i:\n",
    "                if not (sentence == \"\"):\n",
    "                    sentence_list.append(sentence)\n",
    "                    sentence = \"\"\n",
    "                check = True\n",
    "                continue\n",
    "            elif '>' == i:\n",
    "                check = False\n",
    "                continue\n",
    "            else:\n",
    "                if check == True:\n",
    "                    continue\n",
    "                sentence += i\n",
    "\n",
    "    word_list = []\n",
    "    for line in sentence_list:\n",
    "        #print line\n",
    "        line = line.split()\n",
    "        for w in line:\n",
    "            word_list.append(w.strip())\n",
    "    #print \"----------------------------------------------------\"\n",
    "    #print\n",
    "    #print word_list\n",
    "\n",
    "    #i = 0\n",
    "    #for w in word_list:\n",
    "    #    print \"%d : \"%i + w\n",
    "    #    i += 1\n",
    "\n",
    "    #print \"단어 개수 : %d\" % len(word_list)\n",
    "\n",
    "    #print string.punctuation\n",
    "\n",
    "    ch_w_l = []\n",
    "    for w in word_list:\n",
    "        w = ''.join([c for c in w if c not in string.punctuation])\n",
    "        if w == '' or w == '\\n':\n",
    "            continue\n",
    "        ch_w_l.append(w)\n",
    "        #print w\n",
    "    #print \n",
    "    #print \"단어 개수 : %d\" % len(ch_w_l)\n",
    "    #print\n",
    "    #print \"-\" * 30\n",
    "\n",
    "    word_count = {}\n",
    "    for w in ch_w_l:\n",
    "        if w not in word_count:\n",
    "            word_count[w] = 1\n",
    "        else:\n",
    "            word_count[w] += 1\n",
    "\n",
    "    for w in word_count.keys():\n",
    "        if w in stop_word_list:\n",
    "            del word_count[w]\n",
    "        elif w.lower() in stop_word_list:\n",
    "            del word_count[w]\n",
    "            \n",
    "    print \"Site Name : %s\" % site \n",
    "    print \n",
    "    #for k, v in word_count.items():\n",
    "    #    print \"%s : %d\" % (k, v)\n",
    "    #print \n",
    "    print \"단어 종류 : %d\" % len(word_count.keys())    \n",
    "    print \n",
    "    \n",
    "    return word_count\n",
    "\n",
    "site = []\n",
    "site.append(\"http://edition.cnn.com\")  # CNN 홈페이지\n",
    "site.append(\"http://www.nike.com\")  # 외국 나이키 홈페이지\n",
    "site.append(\"https://www.oracle.com\")  # ORACLE 홈페이지\n",
    "site.append(\"http://mlb.mlb.com\")       # MLB 홈페이지\n",
    "site.append(\"https://www.whitehouse.gov\")   # 백악관 홈페이지\n",
    "\n",
    "word_count_list = []\n",
    "for i in range(len(site)):\n",
    "    word_count_list.append(word_list_dic(site[i]))\n",
    "\n",
    "###\n",
    "for i in range(len(site)):\n",
    "    print site[i]\n",
    "    \n",
    "    s = urllib2.urlopen(site[i]).read()\n",
    "    \n",
    "    site_file = \"\"\n",
    "    s_url = \"\"\n",
    "    \n",
    "    if \"http://\" in site[i]:\n",
    "        s_url = site[i].replace(\"http://\", \"\")\n",
    "    if \"https://\" in site[i]:\n",
    "        s_url = site[i].replace(\"https://\", \"\")\n",
    "        \n",
    "    site_file = s_url + \".html\"\n",
    "    \n",
    "    f = open(site_file, 'w')\n",
    "    f.write(s)\n",
    "    f.close()\n",
    "    \n",
    "for i in range(len(site)):\n",
    "    s_url = \"\"\n",
    "    if \"http://\" in site[i]:\n",
    "        s_url = site[i].replace(\"http://\", \"\")\n",
    "    if \"https://\" in site[i]:\n",
    "        s_url = site[i].replace(\"https://\", \"\")\n",
    "    \n",
    "    word_file = s_url + \".words.frequency\"\n",
    "    \n",
    "    f = open(word_file, 'w')\n",
    "    pickle.dump(word_count_list[i], f)\n",
    "    f.close()\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site : edition.cnn.com.words.frequency / Top words : {'function': 24, 'TV': 15, 'CNN': 24}\n",
      "Site : www.nike.com.words.frequency / Top words : {'FRAN\\xc3\\x87AIS': 5, 'DEUTSCH': 4, 'ENGLISH': 13}\n",
      "Site : www.oracle.com.words.frequency / Top words : {'Learn': 32, 'Oracle': 70, 'Cloud': 50}\n",
      "Site : mlb.mlb.com.words.frequency / Top words : {'Game': 39, 'Cubs': 38, 'Oct': 42}\n",
      "Site : www.whitehouse.gov.words.frequency / Top words : {'House': 14, 'amp': 22, 'White': 14}\n"
     ]
    }
   ],
   "source": [
    "f_div_list = []\n",
    "for i in range(len(site)):\n",
    "    s_url = \"\"\n",
    "    if \"http://\" in site[i]:\n",
    "        s_url = site[i].replace(\"http://\", \"\")\n",
    "    if \"https://\" in site[i]:\n",
    "        s_url = site[i].replace(\"https://\", \"\")\n",
    "        \n",
    "    word_file = s_url + \".words.frequency\"\n",
    "    f = open(word_file, 'r')\n",
    "    \n",
    "    t = ()\n",
    "    num_top_d = {}\n",
    "    \n",
    "    f_dic = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    for k, v in f_dic.items():\n",
    "        #print num_top_d\n",
    "        \n",
    "        if len(num_top_d.keys()) < 3:\n",
    "            num_top_d[k] = v\n",
    "        elif min(num_top_d.values()) < v:\n",
    "            #print v\n",
    "            min_num = min(num_top_d.values())\n",
    "            for nk, nv in num_top_d.items():\n",
    "                if min_num == nv:\n",
    "                    del num_top_d[nk]\n",
    "                    break\n",
    "            num_top_d[k] = v\n",
    "    print \"Site : %s / Top words : %s\" % (word_file, num_top_d)\n",
    "    t = (site[i], num_top_d)\n",
    "    f_div_list.append(t)\n",
    "    \n",
    "#for line in f_div_list:\n",
    "#    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like Google : func\n",
      "\n",
      "http://edition.cnn.com : [816]\n",
      "\n",
      "http://www.nike.com : [442]\n",
      "\n",
      "https://www.oracle.com : [1700]\n",
      "\n",
      "http://mlb.mlb.com : [1428]\n",
      "\n",
      "https://www.whitehouse.gov : [476]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Site: https://www.oracle.com\n",
      "Site: http://mlb.mlb.com\n",
      "Site: http://edition.cnn.com\n",
      "Site: https://www.whitehouse.gov\n",
      "Site: http://www.nike.com\n"
     ]
    }
   ],
   "source": [
    "def add(args):\n",
    "    n = 0\n",
    "    \n",
    "    for i in args:\n",
    "        n += int(i) \n",
    "        \n",
    "    return n\n",
    "\n",
    "def similarity_check(search_word, top_word, num):\n",
    "    s_w = search_word\n",
    "    s_len = len(search_word)\n",
    "    i = 0\n",
    "    n = 0\n",
    "    dic = {}\n",
    "    while i < s_len:\n",
    "        n = 0\n",
    "        \n",
    "        while 1:  \n",
    "            point = 0\n",
    "            if (s_len-i+n > s_len):\n",
    "                break\n",
    "            w_s = s_w[i+n:s_len-i+n]\n",
    "            \n",
    "            if w_s == top_word:\n",
    "                point += 1000 * ((1 + s_len - i) * 5) * num\n",
    "            else:\n",
    "                if w_s in top_word: \n",
    "                    point += ((1 + s_len - i) * 2) * num \n",
    "            n += 1    \n",
    "            \n",
    "            if w_s in dic.keys(): \n",
    "                dic[w_s] += point\n",
    "            else:\n",
    "                dic[w_s] = point\n",
    "        i += 1\n",
    "        \n",
    "    for k, v in dic.items():\n",
    "        if max(dic.values()) == v:\n",
    "            return k, v\n",
    "\n",
    "print \"Like Google : \",\n",
    "search = raw_input()\n",
    "similarity = {}\n",
    "\n",
    "for site, top_word in f_div_list:\n",
    "    print \n",
    "    print site , \":\",\n",
    "    s_dic = {}\n",
    "    \n",
    "    for word, word_num in top_word.items():\n",
    "        s_c = 0\n",
    "        w, s = similarity_check(search, word, word_num)\n",
    "        s_dic[w] = s\n",
    "    \n",
    "    print s_dic.values()\n",
    "    for k, v in s_dic.items():\n",
    "        if max(s_dic.values()) == v:\n",
    "            similarity[site] = add(s_dic.values())\n",
    "print \n",
    "print \"=\" * 50\n",
    "print \n",
    "\n",
    "max_v = similarity.values()\n",
    "for i in range(len(max_v)):\n",
    "    for j in range(i, len(max_v)):\n",
    "        if max_v[j] > max_v[i]:\n",
    "            max_v[i], max_v[j] = max_v[j], max_v[i]\n",
    "\n",
    "            \n",
    "for i in max_v:\n",
    "    for k, v in similarity.items():\n",
    "        if i == v:\n",
    "            print \"Site: %s\" % k\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "## 조건 7)\n",
    "\n",
    "- 1번 - 유사도 제어의 어려움   \n",
    "    - 임의로 유사도 방식을 만들긴 했으나, 문제가 많이 있는 것 같습니다. 하지만 그에 반해서 제어가 정말 어려워서, 제가 생각하는대로 잘 나오지 못한 부분이 있습니다.  \n",
    "    \n",
    "    \n",
    "- 2번 - 사이트의 source 코드를 나누는 문제\n",
    "    - 일단 Assignment 2의 마지막 문제에서 원하는 대로, 한 줄씩 코드를 나누어서 리스트를 만들었습니다. 하지만, 보시다시피 CNN 홈페이지 source는 한줄씩 정형화 되어 코드를 만들지 않아서, 한줄씩 나누어 리스트를 만드는 것과 같은 방식이 적절하지 못하다는 것을 발견하였습니다. 즉, 한 줄씩 나누는 것이 적절하지 못하기때문에, <>문자<> 로 나누는 것 또한 적절하지 못한 것 같습니다.  \n",
    "    \n",
    "      \n",
    "- 3번 - 학습성이 없다는 문제\n",
    "    - 일단 파일형태로 저장은 하지만, 이후 source 코드가 변경되면, 이전에 어떤 문자가 높았는지에 대해서 알길이 없어집니다. 즉, 그때 그때 적용될 때는 적절하지만 지속적으로 사용을 해야하는 검색엔진으로 부족할 수 밖에 없다고 생각합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "## 전체 설명\n",
    "\n",
    "> 문제에 명시되었던 이전 Assignment 들을 전부 다시 가지고 와서 실행합니다. 이는 조건 3)에서 필요한 파일를 만들기 위한 절차로 사용됩니다.\n",
    "\n",
    "> 일단 조건 1)부터 설명드리지만, 영어 불용어를 전부 위의 사이트에서 가지고 와서 문자열로 저장했습니다. 그렇게 얻은 문자열은 띄어쓰기와 개행문자로 split를 해서, 리스트 형태로 만들었습니다. 그리고 추가적으로 파일로 저장하게끔 만들었습니다.(파일은 Test용으로 사용되었습니다.) \n",
    "\n",
    "> 다음 조건 2)는 url의 명으로 만드는 것으로 파일명에서 http:// 로 파일을 만들 경우, 오류가 발생하기 때문에 이러한 문자열을 replace를 사용하여 제거합니다. 그리고 나온 url에 .com 문자열을 합쳐서 파일명을 만들고, 내부에 source 를 저장하게됩니다.\n",
    "\n",
    "> 조건 3)은 이전 Assignment 내용을 다시 실행한 후, 나온 값들을 파일에 저장하게됩니다. 이때 pickle 모듈을 사용해서 저장하며, 저장하는 내용은 명시되어 있지않아서 제가 임의로 사전 형태로 저장하게 되었습니다. \n",
    "\n",
    "> 조건 4)는 조건 2)와 3)을 다를 사이트로 실행해야합니다. 그래서 저는 저번에 했던 코드를 다시 수정해서 한번에 값들이 나올 수 있게 정리해서 5개의 외국 사이트를 임의로 가지고 와서 파일로 저장하게 만들었습니다.\n",
    "\n",
    "> 조건 5)에서는 words.frequency 파일에 들어있는 값들을 다시 객체로 가지고 온 후, 출현 빈도가 높은 단어 3개씩을 뽑아서 리스트로 저장했습니다. 리스트의 형태는 내부에 튜플로 저장하는데, 앞에는 사이트명,그리고 뒤는 사전 형태의 단어와 빈도수가 저장되어 있습니다. \n",
    "\n",
    "> 조건 6)에서는 사용자가 임의로 입력한 단어를 제가 임의의 방식으로 유사도를 결정한 후, 유사도의 크기에 따라 site를 나열하게 되어있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 느낀점\n",
    "\n",
    "> 대단히 어렵고 장시간을 요구하는 문제들이 많아서, 과제도 끝나기 직전에 끝났습니다. 일단 오일러 문제자체도 어려운데, 1번, 2번 문제마저도 오래걸리는 문제들뿐이라 더욱 힘들었습니다. 그리고 마지막 문제는 특히 더 힘들었습니다. 하지만 문제의 난이도보다 더 힘든것은 컴퓨터의 사양이 안좋으면 문제를 풀기 더 힘들다는 점이였습니다. 제 컴퓨터의 사양은 별로 높지 않은데 코드는 길어지게 되니 자연스럽게 쥬피터 자체가 느려지고, 다운 현상도 오게 되었습니다.\n",
    "\n",
    "> 문제의 난이도도 난이도지만, 이러한 사양에 대한 문제에 대해서도 조금 생각해주셨으면 좋겠습니다. 읽느라 수고하셨습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
